{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2360, -0.4963, -0.9252, -0.6967,  1.3846,  1.2064, -0.1397,  0.6095,\n",
      "          0.0479, -1.0951],\n",
      "        [-1.3761,  0.5527,  2.1304, -0.8076,  0.5602, -0.4411, -0.7085, -1.4120,\n",
      "          0.3977, -0.1326],\n",
      "        [ 0.2016, -1.1143, -2.6126, -0.0710,  0.3539,  1.8120,  1.4442,  0.4942,\n",
      "          0.0725,  0.2497],\n",
      "        [-0.1954,  1.1863, -0.5781,  0.5903, -2.5749,  0.1307, -0.8420, -1.2042,\n",
      "         -0.3275,  0.7881],\n",
      "        [-0.1124,  0.0141, -0.0396, -2.2365,  0.7088,  1.5085, -1.1258,  0.3626,\n",
      "          0.1687,  0.2565],\n",
      "        [ 0.2092,  0.9054,  0.0553, -0.5218,  0.7049, -0.4590, -0.1547, -2.6150,\n",
      "         -0.3132, -1.7147],\n",
      "        [-0.2978,  0.0990,  0.7797,  0.8806, -1.8936, -0.4920,  0.0229,  1.3465,\n",
      "         -1.2255, -0.4604],\n",
      "        [-0.5694,  0.8669, -0.1855,  0.5933, -0.2396,  1.0078, -1.4239, -0.4348,\n",
      "         -0.0951, -0.2933]])\n",
      "X size: torch.Size([8, 10])\n",
      "Weight: tensor([[ 0.0995,  0.1253, -0.1493, -0.1785,  0.2224, -0.0281,  0.2241, -0.0306,\n",
      "          0.1632, -0.3075],\n",
      "        [-0.0290,  0.2709, -0.1299, -0.2723, -0.2038,  0.3103, -0.1204, -0.2079,\n",
      "          0.2206, -0.1968],\n",
      "        [-0.3083, -0.0042, -0.2312,  0.0364,  0.0451, -0.1307, -0.2897,  0.2218,\n",
      "          0.1244,  0.0561]])\n",
      "Bias: tensor([ 0.2095, -0.1465,  0.2771])\n",
      "Weight size: torch.Size([10, 3])\n",
      "bias size: torch.Size([3])\n",
      "result: tensor([[0.7505, 0.5502, 0.5279],\n",
      "        [0.5237, 0.5568, 0.5484],\n",
      "        [0.6823, 0.5212, 0.5780],\n",
      "        [0.3308, 0.6866, 0.5829],\n",
      "        [0.6001, 0.6996, 0.6262],\n",
      "        [0.7540, 0.6840, 0.4010],\n",
      "        [0.3582, 0.3360, 0.5776],\n",
      "        [0.4677, 0.6519, 0.6590]], grad_fn=<SigmoidBackward>)\n",
      "result size:torch.Size([8, 3])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# n X n_featurex matrix\n",
    "N, n_features = 8, 10\n",
    "\n",
    "X = torch.randn(size=(N,n_features))\n",
    "print(X)\n",
    "print(f'X size: {X.size()}')\n",
    "\n",
    "\n",
    "# Dense layer에는 3개의 뉴런이 있다고 생각하면된다. \n",
    "n_nerons = 3\n",
    "dense = nn.Sequential(\n",
    "    nn.Linear(10, n_nerons),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "# X 값이 dense layer를 통과하면 3개의 activation까지 통과한 값이 나온다. \n",
    "y = dense(X)\n",
    "\n",
    "Weight = dense[0].weight.data\n",
    "bias = dense[0].bias.data\n",
    "\n",
    "print(f'Weight: {Weight}')\n",
    "print(f'Bias: {bias}')\n",
    "Weight = Weight.view(10, 3)\n",
    "print(f'Weight size: {Weight.size()}')\n",
    "print(f'bias size: {bias.size()}')\n",
    "\n",
    "print(f'result: {y}')\n",
    "print(f'result size:{y.size()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: tensor([[ 0.0232, -0.7899,  0.5599, -0.1860,  0.6565,  0.3026, -2.1486,  0.7472,\n",
      "         -1.0176,  0.0276],\n",
      "        [ 0.6283, -0.1395,  0.8266, -0.3145,  0.6252,  0.2600,  0.9741, -0.6149,\n",
      "         -1.5251,  0.2519],\n",
      "        [ 2.1348,  0.1510, -0.2340, -1.0625,  0.7594,  0.6199, -0.7985,  2.4706,\n",
      "         -1.8592, -1.4613],\n",
      "        [-2.4972, -0.0122, -0.9224, -1.1253,  0.3268,  0.3808, -0.5730,  0.5694,\n",
      "          0.5994,  2.2617]]), X shape: torch.Size([4, 10])\n",
      "\n",
      "Dense layer1 Weight shape: torch.Size([10, 3]), bias shape: torch.Size([3])\n",
      "A1: tensor([[0.2596, 0.4915, 0.3826],\n",
      "        [0.4705, 0.5631, 0.6452],\n",
      "        [0.1093, 0.3562, 0.3799],\n",
      "        [0.4166, 0.7936, 0.4099]], grad_fn=<SigmoidBackward>), A1 shape: torch.Size([4, 3])\n",
      "\n",
      "Dense layer2 Weight shape: torch.Size([3, 5]), bias shape: torch.Size([5])\n",
      "y: tensor([[0.4231, 0.5353, 0.4602, 0.6006, 0.5885],\n",
      "        [0.3726, 0.5451, 0.4381, 0.6379, 0.6107],\n",
      "        [0.4398, 0.5434, 0.4535, 0.5782, 0.5957],\n",
      "        [0.3956, 0.5122, 0.4830, 0.6346, 0.5753]], grad_fn=<SigmoidBackward>), y shape: torch.Size([4, 5])\n"
     ]
    }
   ],
   "source": [
    "N, n_features = 4, 10 \n",
    "\n",
    "X_data =  torch.randn(size=(N, n_features))\n",
    "\n",
    "\n",
    "n_nerons = [3, 5]\n",
    "\n",
    "dense1 = nn.Sequential(\n",
    "    nn.Linear(10, n_nerons[0]),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "dense2 = nn.Sequential(\n",
    "    nn.Linear(n_nerons[0], n_nerons[1]),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "A1 = dense1(X_data)\n",
    "y = dense2(A1)\n",
    "\n",
    "### get Weight bias \n",
    "\n",
    "dense1_weight, dense1_bias = dense1[0].weight.data, dense1[0].bias.data\n",
    "dense2_weight, dense2_bias = dense2[0].weight.data, dense2[0].bias.data\n",
    "\n",
    "dense1_weight = dense1_weight.view(10, -1)\n",
    "dense2_weight = dense2_weight.view(3,-1)\n",
    "\n",
    "print(f'X: {X_data}, X shape: {X_data.size()}')\n",
    "print()\n",
    "print(f'Dense layer1 Weight shape: {dense1_weight.size()}, bias shape: {dense1_bias.size()}')\n",
    "print(f'A1: {A1}, A1 shape: {A1.size()}')\n",
    "print()\n",
    "print(f'Dense layer2 Weight shape: {dense2_weight.size()}, bias shape: {dense2_bias.size()}')\n",
    "print(f'y: {y}, y shape: {y.size()}')\n",
    "\n",
    "\n",
    "##  결과들의 데이터 shape를 보면 N x n_feature중에 N 값들은 변하지 않았다. 즉 mini batch size는 변하지 않았다는것 \n",
    "#dense layer 1은 3개의 뉴런을 배치하였고 x_data와 dot product를 하고 난 결과가 4, 3 행렬 3 뉴런의 갯수가 뽑혔고 \n",
    "#dense layer 2는 dense layer1에서의 output인 n x L1개 뉴런의 갯수만큼 데이터가 뽑혀서 인풋으로 들어간서 최종 n x 5 뉴런의 갯수만큼 뽑혔다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Implementation with Sequential Method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(10, 10),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(10, 20),\n",
    "    nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Implementation with Subclassing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.nn' has no attribute 'moduels'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-191-bbf6726d485f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmoduels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         self.dense = nn.Sequential(\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torch.nn' has no attribute 'moduels'"
     ]
    }
   ],
   "source": [
    "class Model(nn.Mo):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Linear(10, 10),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(10, 20),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "test_model = Model(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propagation of Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 20])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Sequential method \n",
    "\n",
    "X = torch.randn(size=(4, 10))\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(10, 10),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(10, 20),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "y = model(X)\n",
    "\n",
    "## Model- subclassing method\n",
    "class models(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(models, self).__init__()\n",
    "        \n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Linear(10, 10),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(10, 20),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        output = self.dense(x)\n",
    "        return output\n",
    "\n",
    "y_model = models()\n",
    "y_model.forward(X).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleAttributeError",
     "evalue": "'Sequential' object has no attribute 'layer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleAttributeError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-207-9e0048345022>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\yym1\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    777\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m         raise ModuleAttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[1;32m--> 779\u001b[1;33m             type(self).__name__, name))\n\u001b[0m\u001b[0;32m    780\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Module'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleAttributeError\u001b[0m: 'Sequential' object has no attribute 'layer'"
     ]
    }
   ],
   "source": [
    "model.layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
